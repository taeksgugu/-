{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f5a078",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "url = 'https://www.chosun.com/nsearch/?query=%EA%B0%9C%EC%9D%B8%EC%A0%95%EB%B3%B4%EB%B3%B4%ED%98%B8%EB%B2%95&siteid=&sort=1&date_period=all&date_start=&date_end=&writer=&field=&emd_word=%EA%B0%9C%EC%9D%B8%EC%A0%95%EB%B3%B4%EB%B3%B4%ED%98%B8%EB%B2%95&expt_word=&opt_chk=false&app_check=0&website=www,chosun&category='\n",
    "driver = Chrome('C:\\\\Users\\\\onrik\\\\chromedriver.exe')\n",
    "driver.get(url)\n",
    "#一共有1542条结果\n",
    "\n",
    "for i in range(1542//10):\n",
    "    print(i)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-1000)\")\n",
    "    driver.find_element_by_xpath('//*[@id=\"load-more-stories\"]').click()\n",
    "    time.sleep(10)\n",
    "response = driver.page_source\n",
    "one_page = BeautifulSoup(response,'html.parser')\n",
    "lst = one_page.find_all(\"div\",{'class':\"story-card-wrapper\"})\n",
    "newslst = []\n",
    "for i in range(len(lst)):\n",
    "    data = lst[i]\n",
    "    try:\n",
    "        title =data.find(\"div\",{\"class\":\"story-card__headline | box--margin-none text--black font--primary h3 text--left\"}).text\n",
    "        date = data.find(\"div\",{\"class\":\"story-card__breadcrumb | text--grey-60 font--primary font--size-sm-14 font--size-md-14 box--margin-bottom-sm text--line-height-1.43\"}).find_all(\"span\")[-1].text\n",
    "    except AttributeError:\n",
    "        date = ''\n",
    "        title = ''\n",
    "    url = data.find(\"a\")['href']\n",
    "#     date = data.find(\"div\",{\"class\":\"story-card__breadcrumb | text--grey-60 font--primary font--size-sm-14 font--size-md-14 box--margin-bottom-sm text--line-height-1.43\"}).find_all(\"span\")[-1].text\n",
    "    newslst.append({\"title\":title, 'url':url, 'date':date})\n",
    "    print(i)\n",
    "newsdf = pd.DataFrame(newslst)\n",
    "newsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a19e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "urldf = pd.read_excel(\"조선일보 url crawling_0101.xlsx\")\n",
    "urldf = urldf[:-10]\n",
    "urldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "urldf['분류'] = urldf['url'].apply(lambda x: x.split('.')[0])\n",
    "urldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "\n",
    "driver = Chrome('C:\\\\Users\\\\onrik\\\\chromedriver.exe', chrome_options=options)\n",
    "\n",
    "for i in range(len(urldf)):\n",
    "    print(i)\n",
    "    if urldf.loc[i,'분류'] == 'https://www' or urldf.loc[i,'분류'] == 'https://biz':\n",
    "        # www, biz\n",
    "        url = urldf.loc[i,'url']\n",
    "        driver.get(url)\n",
    "        response = driver.page_source\n",
    "        one_page = BeautifulSoup(response)\n",
    "        contentlst = one_page.find_all('p')\n",
    "        neirong = ''\n",
    "        for content in contentlst:\n",
    "            neirong += content.text\n",
    "    elif urldf.loc[i,'분류'] == 'http://weekly':\n",
    "        # weekly\n",
    "        url = urldf.loc[i,'url']\n",
    "        response = requests.get(url).text\n",
    "        one_page = BeautifulSoup(response)\n",
    "        neirong = one_page.find('div',{'class':'article_body'}).text\n",
    "    elif urldf.loc[i,'분류'] == 'http://economychosun':\n",
    "        # economychosun\n",
    "        url = urldf.loc[i,'url']\n",
    "        response = requests.get(url).text\n",
    "        one_page = BeautifulSoup(response)\n",
    "        contentlst = one_page.find_all('p')\n",
    "        neirong = ''\n",
    "        for content in contentlst[3:-6]:\n",
    "            neirong += content.text\n",
    "    elif urldf.loc[i,'분류'] == 'http://news':\n",
    "        # news\n",
    "        url = urldf.loc[i,'url']\n",
    "        response = requests.get(url)\n",
    "        driver.get(url)\n",
    "        response = driver.page_source\n",
    "        one_page = BeautifulSoup(response)\n",
    "        neirong = one_page.find('div',{'class':'par'}).text\n",
    "    elif urldf.loc[i,'분류'] == 'http://nsearch':\n",
    "        # nsearch\n",
    "        url = urldf.loc[i,'url']\n",
    "        response = requests.get(url).text\n",
    "        one_page = BeautifulSoup(response)\n",
    "        contentlst = one_page.find_all('div',{\"class\":'par'})\n",
    "        neirong = ''\n",
    "        for content in contentlst:\n",
    "            neirong += content.text\n",
    "    elif urldf.loc[i,'분류'] == 'http://realty' or urldf.loc[i,'분류'] == 'https://health' or urldf.loc[i,'분류'] == 'http://health':\n",
    "        # realty, health\n",
    "        url = urldf.loc[i,'url']\n",
    "        response = requests.get(url)\n",
    "        driver.get(url)\n",
    "        response = driver.page_source\n",
    "        one_page = BeautifulSoup(response)\n",
    "        contentlst = one_page.find_all('div',{'class':'par'})\n",
    "        neirong = ''\n",
    "        for content in contentlst:\n",
    "            neirong += content.text\n",
    "    elif urldf.loc[i,'분류'] == 'http://kid':\n",
    "        # kid\n",
    "        url = urldf.loc[i,'url']\n",
    "        response = requests.get(url)\n",
    "        driver.get(url)\n",
    "        response = driver.page_source\n",
    "        one_page = BeautifulSoup(response)\n",
    "        contentlst = one_page.find_all('div',{'class':'Paragraph'})\n",
    "        neirong = ''\n",
    "        for content in contentlst:\n",
    "            neirong += content.text\n",
    "    elif urldf.loc[i,'분류'] == 'http://edu':\n",
    "        # edu\n",
    "        url = urldf.loc[i,'url']\n",
    "        response = requests.get(url)\n",
    "        response = requests.get(url).text\n",
    "        one_page = BeautifulSoup(response)\n",
    "        contentlst = one_page.find_all('div',{'class':'newsCnt'})\n",
    "        neirong = ''\n",
    "        for content in contentlst:\n",
    "            neirong += content.text\n",
    "    elif urldf.loc[i,'분류'] == 'http://weeklybiz':\n",
    "        # weeklybiz\n",
    "        neirong = 'No page'\n",
    "    elif urldf.loc[i,'분류'] == 'http://newsteacher':\n",
    "        # newsteacher\n",
    "        url = urldf.loc[i,'url']\n",
    "        response = requests.get(url)\n",
    "        driver.get(url)\n",
    "        response = driver.page_source\n",
    "        one_page = BeautifulSoup(response)\n",
    "        contentlst = one_page.find_all('div',{'class':'par'})\n",
    "        neirong = ''\n",
    "        for content in contentlst:\n",
    "            neirong += content.text\n",
    "    neirong = neirong.replace(\"\\n\",'')\n",
    "    neirong = neirong.replace(\"\\t\",'')\n",
    "    neirong = neirong.replace(\"\\r\",'')\n",
    "    neirong = neirong.replace(\"\\xa0\",'')\n",
    "    urldf.loc[i,'content'] = neirong\n",
    "urldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "urldf.to_excel(\"조선일보 crawling_0101.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8efec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
